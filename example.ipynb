{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T22:41:07.211578Z",
     "iopub.status.busy": "2024-02-02T22:41:07.211322Z",
     "iopub.status.idle": "2024-02-02T22:41:08.793036Z",
     "shell.execute_reply": "2024-02-02T22:41:08.792522Z",
     "shell.execute_reply.started": "2024-02-02T22:41:07.211559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gpt2 import GPT2\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337920 this has a different length: 114, padding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "shakespeare_text = './data/shakespeare/input.txt'\n",
    "\n",
    "gpt = GPT2.build(\n",
    "    dataset=\"shakespeare\",\n",
    "    data_path=shakespeare_text,\n",
    "    model_size=\"gpt2\",\n",
    "    max_length=256,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"nampdn-ai/tiny-strange-textbooks\"\n",
    "download = hf_hub_download(repo_id=REPO_ID, filename=\"data_part_0.parquet\", repo_type=\"dataset\", token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet file /home/rafael/.cache/huggingface/hub/datasets--nampdn-ai--tiny-strange-textbooks/snapshots/6066e9c6f9e75e18f3625a551087bd44fe8a84e0/data_part_0.parquet...\n",
      "Tokenizing text...\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT2.build(\n",
    "    dataset=\"tinystrange\",\n",
    "    data_path=download,\n",
    "    model_size=\"gpt2\",\n",
    "    max_length=256,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = gpt.datamodule.dataset_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>len_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ver 2008 R2 installed. I want to use Sync Fra...</td>\n",
       "      <td>[3326, 3648, 371, 17, 6589, 13, 314, 765, 284,...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Introduction to Birthday Invitations\\n...</td>\n",
       "      <td>[11851, 25, 22395, 284, 33511, 10001, 20597, 1...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># Lesson 1: Introduction to Suspense and Dram...</td>\n",
       "      <td>[1303, 12892, 261, 352, 25, 22395, 284, 31922,...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all seasons. The results also showed that the...</td>\n",
       "      <td>[477, 7028, 13, 383, 2482, 635, 3751, 326, 262...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: Introduction to Photinia Rubens - Brig...</td>\n",
       "      <td>[11851, 25, 22395, 284, 5919, 43168, 6256, 641...</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   ver 2008 R2 installed. I want to use Sync Fra...   \n",
       "1   Title: Introduction to Birthday Invitations\\n...   \n",
       "2   # Lesson 1: Introduction to Suspense and Dram...   \n",
       "3   all seasons. The results also showed that the...   \n",
       "4   Title: Introduction to Photinia Rubens - Brig...   \n",
       "\n",
       "                                              tokens  len_tokens  \n",
       "0  [3326, 3648, 371, 17, 6589, 13, 314, 765, 284,...         102  \n",
       "1  [11851, 25, 22395, 284, 33511, 10001, 20597, 1...         232  \n",
       "2  [1303, 12892, 261, 352, 25, 22395, 284, 31922,...         229  \n",
       "3  [477, 7028, 13, 383, 2482, 635, 3751, 326, 262...          92  \n",
       "4  [11851, 25, 22395, 284, 5919, 43168, 6256, 641...         254  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.datamodule.dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-02T23:38:30.876480Z",
     "iopub.status.busy": "2024-02-02T23:38:30.875727Z",
     "iopub.status.idle": "2024-02-03T00:00:23.669308Z",
     "shell.execute_reply": "2024-02-03T00:00:23.668477Z",
     "shell.execute_reply.started": "2024-02-02T23:38:30.876448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-03-06 14:02:44.544135: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-06 14:02:44.755087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 14:02:46.053224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | Transformer | 163 M \n",
      "--------------------------------------\n",
      "163 M     Trainable params\n",
      "0         Non-trainable params\n",
      "163 M     Total params\n",
      "652.149   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 19/80000 [00:12<14:27:14,  1.54it/s, v_num=3, train_loss=8.360]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/miniconda3/envs/gengar/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "gpt.train(\n",
    "    max_epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-03T00:00:30.653875Z",
     "iopub.status.busy": "2024-02-03T00:00:30.653211Z",
     "iopub.status.idle": "2024-02-03T00:00:37.489188Z",
     "shell.execute_reply": "2024-02-03T00:00:37.488583Z",
     "shell.execute_reply.started": "2024-02-03T00:00:30.653850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hence show, twice each other.\n",
      "\n",
      "KING RICHARD II:\n",
      "The worthy danger, my sovereign, I am wisThe senators of the market.\n",
      "Nay,\n",
      "Than'd.\n",
      "\n",
      "QUEENRY VI:\n",
      "Whilstmen, my lord?\n",
      "Come,\n",
      "Come thou be my own disgrace in arms once\n",
      "Must are put Henry's utter it be heir,\n",
      "The waste gave me leaveceund,\n",
      "Whose.\n",
      "\n",
      "The dost thou hast thou art thy dearchurch of thy lord,\n",
      "KING RICHARD III:\n",
      "The dost thou can afford\n",
      "Father, and move,\n",
      "O,\n",
      "Than, my tongue.\n",
      "Not your grace, and thy daughter gone to-kind both, in joy,\n",
      "KING RICHARD II:\n",
      "Ay, and he shall soon\n",
      "God save their tender-believing and afterENCE:\n",
      "As I were a bird with thy head.\n",
      "Flatter my cheeks, my lord.\n",
      "\n",
      "My thoughts, my kingdom!-- eye!\n",
      "\n",
      "Come,\n",
      "\n",
      "KING RICHARD II:\n",
      " shine heaven's face\n",
      "QUEENRY BOLINGHAM:\n",
      "\n",
      "WARWICK:\n",
      "OKE:\n",
      "Henry, doom of us twain a thousand hair it.\n",
      "I have given the adultous sighs, in my mind hath sworn his lands.\n",
      "\n",
      "Up as thou think on, mistrust;\n",
      "JOHN OF YORK:\n",
      "HENRY VI:\n",
      "So do it out of Norfolk:\n",
      "For once again.\n",
      "Which thou saw him they shall breed thy hand where he shall free to spite\n",
      "Ay,\n",
      "\n",
      "Henry, my name,\n",
      "HENRY VI:\n",
      "ares Richard,\n",
      "So high\n",
      "Nurse:\n",
      "That bear.\n",
      "Then vouchsafe to France?\n",
      "HENRY BOLINGBROKE:\n",
      "Each deep wherein told me up my sovereign's truen'd with all despair, ere Richard's lament of this frail.\n",
      "\n",
      "Five have IYe shall be buried.\n",
      "unexperienced, if thou hast marvellous tender hearts!\n",
      "The worst of thy dog'st thou shalt not kill'd and these witnesses.\n",
      "Was the realm;\n",
      "HENRY PERCY:\n",
      " bal,\n",
      "HENRY BOLINGBRO, good! my uncle eyes I'll great Clarence,\n",
      " person as thou shalt be so blunt to claim his bosom\n",
      "Lady:\n",
      "HEN SCROOP:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gpt.generate(prompt=\"If you tickle us do we not laugh?\", max_len=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337920 this has a different length: 114, padding\n"
     ]
    }
   ],
   "source": [
    "from gpt2.modules.data import TextDataset\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "dataset = TextDataset.from_file(\"./data/shakespeare/input.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
       "          3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
       "           461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
       "          1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
       "           680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
       "         12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
       "           345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
       "           284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
       "           760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
       "         22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
       "          1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
       "           470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
       "           517,  3375,   319,   470,    26,  1309,   340,   307]),\n",
       " tensor([22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,  3285,\n",
       "           502,  2740,    13,   198,   198,  3237,    25,   198,  5248,   461,\n",
       "            11,  2740,    13,   198,   198,  5962, 22307,    25,   198,  1639,\n",
       "           389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,   680,\n",
       "            30,   198,   198,  3237,    25,   198,  4965,  5634,    13, 12939,\n",
       "            13,   198,   198,  5962, 22307,    25,   198,  5962,    11,   345,\n",
       "           760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,   284,\n",
       "           262,   661,    13,   198,   198,  3237,    25,   198,  1135,   760,\n",
       "           470,    11,   356,   760,   470,    13,   198,   198,  5962, 22307,\n",
       "            25,   198,  5756,   514,  1494,   683,    11,   290,   356,  1183,\n",
       "           423, 11676,   379,   674,   898,  2756,    13,   198,  3792,   470,\n",
       "           257, 15593,    30,   198,   198,  3237,    25,   198,  2949,   517,\n",
       "          3375,   319,   470,    26,  1309,   340,   307,  1760]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
