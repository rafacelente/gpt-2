{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, GELU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the Masked Self Attention\n",
    "\n",
    "![Self Attention](images/self_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2attention import SelfAttention\n",
    "\n",
    "sa = SelfAttention(768, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 50, 768)\n",
    "sa.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 128\n",
    "dim = 768\n",
    "n_heads = 12\n",
    "embed_dim = 768\n",
    "context = 50 # simulating the 50th word in the context\n",
    "batch_size = 4\n",
    "\n",
    "c_attn = Linear(dim, dim*3, bias=True) # W_q, W_k, W_v, that's why dim*3\n",
    "c_proj = Linear(dim, dim, bias=True)\n",
    "\n",
    "x = torch.randn(batch_size, context, embed_dim) # (batch_size, context, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x):\n",
    "    return x.view(x.shape[0], x.shape[1], n_heads, dim//n_heads)\n",
    "\n",
    "def merge_heads(x: torch.Tensor, num_heads, head_dim) -> torch.Tensor:\n",
    "        x = x.contiguous()\n",
    "        return x.view((x.shape[0], x.shape[1], num_heads * head_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None):\n",
    "    w = torch.matmul(q.transpose(1,2), k.transpose(1, 2).transpose(2, 3))\n",
    "    w = w / torch.sqrt(torch.tensor(k.shape[-1]).float())\n",
    "    print(f'w.shape: {w.shape}')\n",
    "    print(f'q.shape: {q.shape}')\n",
    "    print(f'k.shape: {k.shape}')\n",
    "    print(f'v.shape: {v.shape}')\n",
    "    if mask is not None:\n",
    "        w = w + mask\n",
    "    query_len = q.shape[1]\n",
    "    key_len = k.shape[1]\n",
    "    # Implementing the mask\n",
    "    causal_mask = torch.tril(torch.ones((query_len, key_len), dtype=torch.bool))\n",
    "    mask_value = torch.finfo(w.dtype).min # represent -inf\n",
    "    w = torch.where(causal_mask, w, mask_value)\n",
    "    print(f'w.shape: {w.shape}')\n",
    "    \n",
    "    w = F.softmax(w, dim=-1)\n",
    "    print(f'w.shape after softmax: {w.shape}')\n",
    "    print(f'v.shape: {v.shape}')\n",
    "    attn_output = torch.matmul(w, v.transpose(1, 2)).transpose(1, 2)\n",
    "    print(f'attn_output.shape: {attn_output.shape}')\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.shape: torch.Size([4, 12, 50, 50])\n",
      "q.shape: torch.Size([4, 50, 12, 64])\n",
      "k.shape: torch.Size([4, 50, 12, 64])\n",
      "v.shape: torch.Size([4, 50, 12, 64])\n",
      "w.shape: torch.Size([4, 12, 50, 50])\n",
      "w.shape after softmax: torch.Size([4, 12, 50, 50])\n",
      "v.shape: torch.Size([4, 50, 12, 64])\n",
      "attn_output.shape: torch.Size([4, 50, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward operation\n",
    "xqkv = c_attn(x)\n",
    "queries, keys, values = xqkv.split(dim, dim=2)\n",
    "queries = split_heads(queries)\n",
    "keys = split_heads(keys)\n",
    "values = split_heads(values)\n",
    "\n",
    "attn_output = attention(queries, keys, values)\n",
    "attn_output = merge_heads(attn_output, n_heads, dim//n_heads)\n",
    "attn_output = c_proj(attn_output)\n",
    "attn_output.shape\n",
    "\n",
    "# 4,50,768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 616, 3290, 318, 13779]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hello, my dog is cute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T23:39:44.352668Z",
     "iopub.status.busy": "2024-01-15T23:39:44.352042Z",
     "iopub.status.idle": "2024-01-15T23:39:46.555869Z",
     "shell.execute_reply": "2024-01-15T23:39:46.555331Z",
     "shell.execute_reply.started": "2024-01-15T23:39:44.352642Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "from dataset import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "model_size = \"gpt2\"\n",
    "tokenizer = tiktoken.get_encoding(model_size)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = Transformer(\n",
    "            dim=768,\n",
    "            n_heads=12,\n",
    "            vocab_size=50257,\n",
    "            n_layers=12,\n",
    "            max_seq_len=128,\n",
    "            device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T23:39:47.647758Z",
     "iopub.status.busy": "2024-01-15T23:39:47.646876Z",
     "iopub.status.idle": "2024-01-15T23:39:47.819507Z",
     "shell.execute_reply": "2024-01-15T23:39:47.819038Z",
     "shell.execute_reply.started": "2024-01-15T23:39:47.647731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337920 this has a different length: 114, padding\n",
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "input_file_path = os.path.join(\"./data/shakespeare/\", 'input.txt')\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "dataset = TextDataset(data, tokenizer, max_length=128, input_type=\"text\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T23:41:18.336020Z",
     "iopub.status.busy": "2024-01-15T23:41:18.335319Z",
     "iopub.status.idle": "2024-01-15T23:52:07.572231Z",
     "shell.execute_reply": "2024-01-15T23:52:07.571661Z",
     "shell.execute_reply.started": "2024-01-15T23:41:18.335994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0: loss: 11.241231918334961\n",
      "epoch 0 batch 200: loss: 5.63091516494751\n",
      "epoch 0 batch 400: loss: 5.092859268188477\n",
      "epoch 0 batch 600: loss: 4.970579147338867\n",
      "epoch 1 batch 0: loss: 4.348081588745117\n",
      "epoch 1 batch 200: loss: 4.332835674285889\n",
      "epoch 1 batch 400: loss: 3.9893200397491455\n",
      "epoch 1 batch 600: loss: 4.3360981941223145\n",
      "epoch 2 batch 0: loss: 3.6253151893615723\n",
      "epoch 2 batch 200: loss: 3.7650766372680664\n",
      "epoch 2 batch 400: loss: 4.161316871643066\n",
      "epoch 2 batch 600: loss: 3.8775484561920166\n",
      "epoch 3 batch 0: loss: 3.393232822418213\n",
      "epoch 3 batch 200: loss: 3.6106767654418945\n",
      "epoch 3 batch 400: loss: 3.3115596771240234\n",
      "epoch 3 batch 600: loss: 3.516842842102051\n",
      "epoch 4 batch 0: loss: 1.9148331880569458\n",
      "epoch 4 batch 200: loss: 2.240736484527588\n",
      "epoch 4 batch 400: loss: 2.2979681491851807\n",
      "epoch 4 batch 600: loss: 2.4867141246795654\n",
      "epoch 5 batch 0: loss: 1.7106881141662598\n",
      "epoch 5 batch 200: loss: 1.4227973222732544\n",
      "epoch 5 batch 400: loss: 1.6919183731079102\n",
      "epoch 5 batch 600: loss: 1.8788992166519165\n",
      "epoch 6 batch 0: loss: 0.9980119466781616\n",
      "epoch 6 batch 200: loss: 1.072981595993042\n",
      "epoch 6 batch 400: loss: 0.9806205630302429\n",
      "epoch 6 batch 600: loss: 1.328201413154602\n",
      "epoch 7 batch 0: loss: 0.48939836025238037\n",
      "epoch 7 batch 200: loss: 0.4545395076274872\n",
      "epoch 7 batch 400: loss: 0.7511530518531799\n",
      "epoch 7 batch 600: loss: 0.6479295492172241\n",
      "epoch 8 batch 0: loss: 0.231238454580307\n",
      "epoch 8 batch 200: loss: 0.2683151662349701\n",
      "epoch 8 batch 400: loss: 0.34405070543289185\n",
      "epoch 8 batch 600: loss: 0.6009459495544434\n",
      "epoch 9 batch 0: loss: 0.2392331063747406\n",
      "epoch 9 batch 200: loss: 0.23276816308498383\n",
      "epoch 9 batch 400: loss: 0.23518045246601105\n",
      "epoch 9 batch 600: loss: 0.3059055507183075\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 200 == 0:\n",
    "            print(f\"epoch {epoch} batch {i}: loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T23:57:22.043591Z",
     "iopub.status.busy": "2024-01-15T23:57:22.043326Z",
     "iopub.status.idle": "2024-01-15T23:57:22.310560Z",
     "shell.execute_reply": "2024-01-15T23:57:22.309923Z",
     "shell.execute_reply.started": "2024-01-15T23:57:22.043571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me to the char of the character of my parent.\n",
      "\n",
      "LUCENTIO:\n",
      "I could not to a slaughter.\n",
      "\n",
      "BAPTISTA:\n",
      "What is to be your daughter,--\n",
      "\n",
      "Prithee, good\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(\n",
    "            prompt,\n",
    "            max_len=50,\n",
    "            do_sample=True, \n",
    "            temperature=0.1, \n",
    "            top_k=0, \n",
    "            top_p=0.9, \n",
    "            repetition_penalty=1.0, \n",
    "            num_return_sequences=1, \n",
    "            batch_size=1, \n",
    "            device=\"cuda\"):\n",
    "        \n",
    "        device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "        prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "        for _ in range(num_return_sequences):\n",
    "            generated = torch.tensor([prompt_tokens])\n",
    "            generated = generated.to(device)\n",
    "            prompt_len = len(prompt_tokens)\n",
    "\n",
    "            for _ in range(max_len):\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(generated)\n",
    "                    next_token_logits = outputs[0][:, -1, :]\n",
    "                    # for token in set(generated[0].tolist()):\n",
    "                    #     next_token_logits[token] /= repetition_penalty\n",
    "                    #next_token_logits = next_token_logits / temperature\n",
    "                    #filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "                    if do_sample:\n",
    "                        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "                    # else:\n",
    "                    #     next_token = torch.argmax(filtered_logits, dim=-1)\n",
    "                    generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            result = generated[0].tolist()\n",
    "            text = tokenizer.decode(result[prompt_len:])\n",
    "            return prompt + text\n",
    "        \n",
    "print(generate(\"Help\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
