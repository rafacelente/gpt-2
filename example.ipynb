{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, GELU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout, functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the Masked Self Attention\n",
    "\n",
    "![Self Attention](images/self_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2attention import SelfAttention\n",
    "\n",
    "sa = SelfAttention(768, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 50, 768)\n",
    "sa.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 128\n",
    "dim = 768\n",
    "n_heads = 12\n",
    "embed_dim = 768\n",
    "context = 50 # simulating the 50th word in the context\n",
    "batch_size = 4\n",
    "\n",
    "c_attn = Linear(dim, dim*3, bias=True) # W_q, W_k, W_v, that's why dim*3\n",
    "c_proj = Linear(dim, dim, bias=True)\n",
    "\n",
    "x = torch.randn(batch_size, context, embed_dim) # (batch_size, context, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x):\n",
    "    return x.view(x.shape[0], x.shape[1], n_heads, dim//n_heads)\n",
    "\n",
    "def merge_heads(x: torch.Tensor, num_heads, head_dim) -> torch.Tensor:\n",
    "        x = x.contiguous()\n",
    "        return x.view((x.shape[0], x.shape[1], num_heads * head_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None):\n",
    "    w = torch.matmul(q.transpose(1,2), k.transpose(1, 2).transpose(2, 3))\n",
    "    w = w / torch.sqrt(torch.tensor(k.shape[-1]).float())\n",
    "    print(f'w.shape: {w.shape}')\n",
    "    print(f'q.shape: {q.shape}')\n",
    "    print(f'k.shape: {k.shape}')\n",
    "    print(f'v.shape: {v.shape}')\n",
    "    if mask is not None:\n",
    "        w = w + mask\n",
    "    query_len = q.shape[1]\n",
    "    key_len = k.shape[1]\n",
    "    # Implementing the mask\n",
    "    causal_mask = torch.tril(torch.ones((query_len, key_len), dtype=torch.bool))\n",
    "    mask_value = torch.finfo(w.dtype).min # represent -inf\n",
    "    w = torch.where(causal_mask, w, mask_value)\n",
    "    print(f'w.shape: {w.shape}')\n",
    "    \n",
    "    w = F.softmax(w, dim=-1)\n",
    "    print(f'w.shape after softmax: {w.shape}')\n",
    "    print(f'v.shape: {v.shape}')\n",
    "    attn_output = torch.matmul(w, v.transpose(1, 2)).transpose(1, 2)\n",
    "    print(f'attn_output.shape: {attn_output.shape}')\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.shape: torch.Size([4, 12, 50, 50])\n",
      "q.shape: torch.Size([4, 50, 12, 64])\n",
      "k.shape: torch.Size([4, 50, 12, 64])\n",
      "v.shape: torch.Size([4, 50, 12, 64])\n",
      "w.shape: torch.Size([4, 12, 50, 50])\n",
      "w.shape after softmax: torch.Size([4, 12, 50, 50])\n",
      "v.shape: torch.Size([4, 50, 12, 64])\n",
      "attn_output.shape: torch.Size([4, 50, 12, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward operation\n",
    "xqkv = c_attn(x)\n",
    "queries, keys, values = xqkv.split(dim, dim=2)\n",
    "queries = split_heads(queries)\n",
    "keys = split_heads(keys)\n",
    "values = split_heads(values)\n",
    "\n",
    "attn_output = attention(queries, keys, values)\n",
    "attn_output = merge_heads(attn_output, n_heads, dim//n_heads)\n",
    "attn_output = c_proj(attn_output)\n",
    "attn_output.shape\n",
    "\n",
    "# 4,50,768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 616, 3290, 318, 13779]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hello, my dog is cute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformer import Transformer\n",
    "from dataset import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "model_size = \"gpt2\"\n",
    "tokenizer = tiktoken.get_encoding(model_size)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = Transformer(\n",
    "            dim=768,\n",
    "            n_heads=12,\n",
    "            vocab_size=50257,\n",
    "            n_layers=12,\n",
    "            max_seq_len=128,\n",
    "            device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_dir = \"./data/shakespeare\"\n",
    "data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.int16, mode='r') \n",
    "dataset = TextDataset(data, tokenizer, max_length=128, input_type=\"bin\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/shakespeare/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_file_path = os.path.join(\"./data/shakespeare/\", 'input.txt')\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "\n",
    "dataset = TextDataset(data, tokenizer, max_length=128, input_type=\"text\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [33],\n",
       " [68],\n",
       " [69],\n",
       " [78],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [220],\n",
       " [79],\n",
       " [81],\n",
       " [78],\n",
       " [66],\n",
       " [68],\n",
       " [68],\n",
       " [67],\n",
       " [220],\n",
       " [64],\n",
       " [77],\n",
       " [88],\n",
       " [220],\n",
       " [69],\n",
       " [84],\n",
       " [81],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [81],\n",
       " [11],\n",
       " [220],\n",
       " [71],\n",
       " [68],\n",
       " [64],\n",
       " [81],\n",
       " [220],\n",
       " [76],\n",
       " [68],\n",
       " [220],\n",
       " [82],\n",
       " [79],\n",
       " [68],\n",
       " [64],\n",
       " [74],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [32],\n",
       " [75],\n",
       " [75],\n",
       " [25],\n",
       " [198],\n",
       " [50],\n",
       " [79],\n",
       " [68],\n",
       " [64],\n",
       " [74],\n",
       " [11],\n",
       " [220],\n",
       " [82],\n",
       " [79],\n",
       " [68],\n",
       " [64],\n",
       " [74],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [56],\n",
       " [78],\n",
       " [84],\n",
       " [220],\n",
       " [64],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [64],\n",
       " [75],\n",
       " [75],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [82],\n",
       " [78],\n",
       " [75],\n",
       " [85],\n",
       " [68],\n",
       " [67],\n",
       " [220],\n",
       " [81],\n",
       " [64],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [81],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [220],\n",
       " [67],\n",
       " [72],\n",
       " [68],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [64],\n",
       " [77],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [220],\n",
       " [69],\n",
       " [64],\n",
       " [76],\n",
       " [72],\n",
       " [82],\n",
       " [71],\n",
       " [30],\n",
       " [198],\n",
       " [198],\n",
       " [32],\n",
       " [75],\n",
       " [75],\n",
       " [25],\n",
       " [198],\n",
       " [49],\n",
       " [68],\n",
       " [82],\n",
       " [78],\n",
       " [75],\n",
       " [85],\n",
       " [68],\n",
       " [67],\n",
       " [13],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [82],\n",
       " [78],\n",
       " [75],\n",
       " [85],\n",
       " [68],\n",
       " [67],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [11],\n",
       " [220],\n",
       " [88],\n",
       " [78],\n",
       " [84],\n",
       " [220],\n",
       " [74],\n",
       " [77],\n",
       " [78],\n",
       " [86],\n",
       " [220],\n",
       " [34],\n",
       " [64],\n",
       " [72],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [44],\n",
       " [64],\n",
       " [81],\n",
       " [66],\n",
       " [72],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [72],\n",
       " [82],\n",
       " [220],\n",
       " [66],\n",
       " [71],\n",
       " [72],\n",
       " [68],\n",
       " [69],\n",
       " [220],\n",
       " [68],\n",
       " [77],\n",
       " [68],\n",
       " [76],\n",
       " [88],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [79],\n",
       " [68],\n",
       " [78],\n",
       " [79],\n",
       " [75],\n",
       " [68],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [32],\n",
       " [75],\n",
       " [75],\n",
       " [25],\n",
       " [198],\n",
       " [54],\n",
       " [68],\n",
       " [220],\n",
       " [74],\n",
       " [77],\n",
       " [78],\n",
       " [86],\n",
       " [6],\n",
       " [83],\n",
       " [11],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [220],\n",
       " [74],\n",
       " [77],\n",
       " [78],\n",
       " [86],\n",
       " [6],\n",
       " [83],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [43],\n",
       " [68],\n",
       " [83],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [74],\n",
       " [72],\n",
       " [75],\n",
       " [75],\n",
       " [220],\n",
       " [71],\n",
       " [72],\n",
       " [76],\n",
       " [11],\n",
       " [220],\n",
       " [64],\n",
       " [77],\n",
       " [67],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [6],\n",
       " [75],\n",
       " [75],\n",
       " [220],\n",
       " [71],\n",
       " [64],\n",
       " [85],\n",
       " [68],\n",
       " [220],\n",
       " [66],\n",
       " [78],\n",
       " [81],\n",
       " [77],\n",
       " [220],\n",
       " [64],\n",
       " [83],\n",
       " [220],\n",
       " [78],\n",
       " [84],\n",
       " [81],\n",
       " [220],\n",
       " [78],\n",
       " [86],\n",
       " [77],\n",
       " [220],\n",
       " [79],\n",
       " [81],\n",
       " [72],\n",
       " [66],\n",
       " [68],\n",
       " [13],\n",
       " [198],\n",
       " [40],\n",
       " [82],\n",
       " [6],\n",
       " [83],\n",
       " [220],\n",
       " [64],\n",
       " [220],\n",
       " [85],\n",
       " [68],\n",
       " [81],\n",
       " [67],\n",
       " [72],\n",
       " [66],\n",
       " [83],\n",
       " [30],\n",
       " [198],\n",
       " [198],\n",
       " [32],\n",
       " [75],\n",
       " [75],\n",
       " [25],\n",
       " [198],\n",
       " [45],\n",
       " [78],\n",
       " [220],\n",
       " [76],\n",
       " [78],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [83],\n",
       " [64],\n",
       " [75],\n",
       " [74],\n",
       " [72],\n",
       " [77],\n",
       " [70],\n",
       " [220],\n",
       " [78],\n",
       " [77],\n",
       " [6],\n",
       " [83],\n",
       " [26],\n",
       " [220],\n",
       " [75],\n",
       " [68],\n",
       " [83],\n",
       " [220],\n",
       " [72],\n",
       " [83],\n",
       " [220],\n",
       " [65],\n",
       " [68],\n",
       " [220],\n",
       " [67],\n",
       " [78],\n",
       " [77],\n",
       " [68],\n",
       " [25],\n",
       " [220],\n",
       " [64],\n",
       " [86],\n",
       " [64],\n",
       " [88],\n",
       " [11],\n",
       " [220],\n",
       " [64],\n",
       " [86],\n",
       " [64],\n",
       " [88],\n",
       " [0],\n",
       " [198],\n",
       " [198],\n",
       " [50],\n",
       " [68],\n",
       " [66],\n",
       " [78],\n",
       " [77],\n",
       " [67],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [46],\n",
       " [77],\n",
       " [68],\n",
       " [220],\n",
       " [86],\n",
       " [78],\n",
       " [81],\n",
       " [67],\n",
       " [11],\n",
       " [220],\n",
       " [70],\n",
       " [78],\n",
       " [78],\n",
       " [67],\n",
       " [220],\n",
       " [66],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [82],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " [37],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [34],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [25],\n",
       " [198],\n",
       " [54],\n",
       " [68],\n",
       " [220],\n",
       " [64],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [64],\n",
       " [66],\n",
       " [66],\n",
       " [78],\n",
       " [84],\n",
       " [77],\n",
       " [83],\n",
       " [68],\n",
       " [67],\n",
       " [220],\n",
       " [79],\n",
       " [78],\n",
       " [78],\n",
       " [81],\n",
       " [220],\n",
       " [66],\n",
       " [72],\n",
       " [83],\n",
       " [72],\n",
       " [89],\n",
       " [68],\n",
       " [77],\n",
       " [82],\n",
       " [11],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [79],\n",
       " [64],\n",
       " [83],\n",
       " [81],\n",
       " [72],\n",
       " [66],\n",
       " [72],\n",
       " [64],\n",
       " [77],\n",
       " [82],\n",
       " [220],\n",
       " [70],\n",
       " [78],\n",
       " [78],\n",
       " [67],\n",
       " [13],\n",
       " [198],\n",
       " [54],\n",
       " [71],\n",
       " [64],\n",
       " [83],\n",
       " [220],\n",
       " [64],\n",
       " [84],\n",
       " [83],\n",
       " [71],\n",
       " [78],\n",
       " [81],\n",
       " [72],\n",
       " [83],\n",
       " [88],\n",
       " [220],\n",
       " [82],\n",
       " [84],\n",
       " [81],\n",
       " [69],\n",
       " [68],\n",
       " [72],\n",
       " [83],\n",
       " [82],\n",
       " [220],\n",
       " [78],\n",
       " [77],\n",
       " [220],\n",
       " [86],\n",
       " [78],\n",
       " [84],\n",
       " [75],\n",
       " [67],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [75],\n",
       " [72],\n",
       " [68],\n",
       " [85],\n",
       " [68],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [25],\n",
       " [220],\n",
       " [72],\n",
       " [69],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [88],\n",
       " [198],\n",
       " [86],\n",
       " [78],\n",
       " [84],\n",
       " [75],\n",
       " [67],\n",
       " [220],\n",
       " [88],\n",
       " [72],\n",
       " [68],\n",
       " [75],\n",
       " [67],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [65],\n",
       " [84],\n",
       " [83],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [82],\n",
       " [84],\n",
       " [79],\n",
       " [68],\n",
       " [81],\n",
       " [69],\n",
       " [75],\n",
       " [84],\n",
       " [72],\n",
       " [83],\n",
       " [88],\n",
       " [11],\n",
       " [220],\n",
       " [86],\n",
       " [71],\n",
       " [72],\n",
       " [75],\n",
       " [68],\n",
       " [220],\n",
       " [72],\n",
       " [83],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [81],\n",
       " [68],\n",
       " [198],\n",
       " [86],\n",
       " [71],\n",
       " [78],\n",
       " [75],\n",
       " [68],\n",
       " [82],\n",
       " [78],\n",
       " [76],\n",
       " [68],\n",
       " [11],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [220],\n",
       " [76],\n",
       " [72],\n",
       " [70],\n",
       " [71],\n",
       " [83],\n",
       " [220],\n",
       " [70],\n",
       " [84],\n",
       " [68],\n",
       " [82],\n",
       " [82],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [88],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [75],\n",
       " [72],\n",
       " [68],\n",
       " [85],\n",
       " [68],\n",
       " [67],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [71],\n",
       " [84],\n",
       " [76],\n",
       " [64],\n",
       " [77],\n",
       " [68],\n",
       " [75],\n",
       " [88],\n",
       " [26],\n",
       " [198],\n",
       " [65],\n",
       " [84],\n",
       " [83],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [88],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [72],\n",
       " [77],\n",
       " [74],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [220],\n",
       " [64],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [78],\n",
       " [220],\n",
       " [67],\n",
       " [68],\n",
       " [64],\n",
       " [81],\n",
       " [25],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [75],\n",
       " [68],\n",
       " [64],\n",
       " [77],\n",
       " [77],\n",
       " [68],\n",
       " [82],\n",
       " [82],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [64],\n",
       " [83],\n",
       " [198],\n",
       " [64],\n",
       " [69],\n",
       " [69],\n",
       " [75],\n",
       " [72],\n",
       " [66],\n",
       " [83],\n",
       " [82],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [11],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [78],\n",
       " [65],\n",
       " [73],\n",
       " [68],\n",
       " [66],\n",
       " [83],\n",
       " [220],\n",
       " [78],\n",
       " [69],\n",
       " [220],\n",
       " [78],\n",
       " [84],\n",
       " [81],\n",
       " [220],\n",
       " [76],\n",
       " [72],\n",
       " [82],\n",
       " [68],\n",
       " [81],\n",
       " [88],\n",
       " [11],\n",
       " [220],\n",
       " [72],\n",
       " [82],\n",
       " [220],\n",
       " [64],\n",
       " [82],\n",
       " [220],\n",
       " [64],\n",
       " [77],\n",
       " [198],\n",
       " [72],\n",
       " [77],\n",
       " [85],\n",
       " [68],\n",
       " [77],\n",
       " [83],\n",
       " [78],\n",
       " [81],\n",
       " [88],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [220],\n",
       " [79],\n",
       " [64],\n",
       " [81],\n",
       " [83],\n",
       " [72],\n",
       " [66],\n",
       " [84],\n",
       " [75],\n",
       " [64],\n",
       " [81],\n",
       " [72],\n",
       " [82],\n",
       " [68],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [72],\n",
       " [81],\n",
       " [220],\n",
       " [64],\n",
       " [65],\n",
       " [84],\n",
       " [77],\n",
       " [67],\n",
       " [64],\n",
       " [77],\n",
       " [66],\n",
       " [68],\n",
       " [26],\n",
       " [220],\n",
       " [78],\n",
       " [84],\n",
       " [81],\n",
       " [198],\n",
       " [82],\n",
       " [84],\n",
       " [69],\n",
       " [69],\n",
       " [68],\n",
       " [81],\n",
       " [64],\n",
       " [77],\n",
       " [66],\n",
       " [68],\n",
       " [220],\n",
       " [72],\n",
       " [82],\n",
       " [220],\n",
       " [64],\n",
       " [220],\n",
       " [70],\n",
       " [64],\n",
       " [72],\n",
       " [77],\n",
       " [220],\n",
       " [83],\n",
       " [78],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [76],\n",
       " [220],\n",
       " [43],\n",
       " [68],\n",
       " [83],\n",
       " [220],\n",
       " [84],\n",
       " [82],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [85],\n",
       " [68],\n",
       " [77],\n",
       " [70],\n",
       " [68],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [72],\n",
       " [82],\n",
       " [220],\n",
       " [86],\n",
       " [72],\n",
       " [83],\n",
       " [71],\n",
       " [198],\n",
       " [78],\n",
       " [84],\n",
       " [81],\n",
       " [220],\n",
       " [79],\n",
       " [72],\n",
       " [74],\n",
       " [68],\n",
       " [82],\n",
       " [11],\n",
       " [220],\n",
       " [68],\n",
       " [81],\n",
       " [68],\n",
       " [220],\n",
       " [86],\n",
       " [68],\n",
       " [220],\n",
       " [65],\n",
       " [68],\n",
       " [66],\n",
       " [78],\n",
       " [76],\n",
       " [68],\n",
       " [220],\n",
       " [81],\n",
       " [64],\n",
       " [74],\n",
       " [68],\n",
       " [82],\n",
       " [25],\n",
       " [220],\n",
       " [69],\n",
       " [78],\n",
       " [81],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [68],\n",
       " [220],\n",
       " [70],\n",
       " [78],\n",
       " [67],\n",
       " [82],\n",
       " [220],\n",
       " [74],\n",
       " [77],\n",
       " [78],\n",
       " [86],\n",
       " [220],\n",
       " [40],\n",
       " [198],\n",
       " [82],\n",
       " [79],\n",
       " [68],\n",
       " [64],\n",
       " [74],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [72],\n",
       " [82],\n",
       " [220],\n",
       " [72],\n",
       " [77],\n",
       " [220],\n",
       " [71],\n",
       " [84],\n",
       " [77],\n",
       " [70],\n",
       " [68],\n",
       " [81],\n",
       " [220],\n",
       " [69],\n",
       " [78],\n",
       " [81],\n",
       " [220],\n",
       " [65],\n",
       " [81],\n",
       " [68],\n",
       " [64],\n",
       " [67],\n",
       " [11],\n",
       " [220],\n",
       " [77],\n",
       " [78],\n",
       " [83],\n",
       " [220],\n",
       " [72],\n",
       " [77],\n",
       " [220],\n",
       " [83],\n",
       " [71],\n",
       " [72],\n",
       " [81],\n",
       " [82],\n",
       " [83],\n",
       " [220],\n",
       " [69],\n",
       " [78],\n",
       " [81],\n",
       " [220],\n",
       " [81],\n",
       " [68],\n",
       " [85],\n",
       " [68],\n",
       " [77],\n",
       " [70],\n",
       " [68],\n",
       " [13],\n",
       " [198],\n",
       " [198],\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[68], [77], [84], [220]]\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(x.tolist())\n",
    "    print(tokenizer.decode(x[1].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "position_embeddings.shape: torch.Size([128, 768])\n",
      "x.shape: torch.Size([4, 128])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Personal/gpt-2/transformer.py:53\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_embeddings.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#+ position_embeddings\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx.shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_f(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs, labels = batch\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "        \n",
    "        print(f\"batch: {i}\")\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, labels=labels)\n",
    "        print(f\"loss: {loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
